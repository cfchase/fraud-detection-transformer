apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: kserve-raw-deployment
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      annotations:
        opendatahub.io/accelerator-name: ''
        opendatahub.io/apiProtocol: REST
        opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
        opendatahub.io/template-display-name: OpenVINO Model Server
        opendatahub.io/template-name: kserve-ovms
        openshift.io/display-name: fraud
      name: fraud
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '8888'
      containers:
        - args:
            - '--model_name={{.Name}}'
            - '--port=8001'
            - '--rest_port=8888'
            - '--model_path=/mnt/models'
            - '--file_system_poll_wait_seconds=0'
            - '--grpc_bind_address=0.0.0.0'
            - '--rest_bind_address=0.0.0.0'
            - '--target_device=AUTO'
            - '--metrics_enable'
          image: 'quay.io/modh/openvino_model_server@sha256:6c7795279f9075bebfcd9aecbb4a4ce4177eec41fb3f3e1f1079ce6309b7ae45'
          name: kserve-container
          ports:
            - containerPort: 8888
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
      multiModel: false
      protocolVersions:
        - v2
        - grpc-v2
      supportedModelFormats:
        - autoSelect: true
          name: openvino_ir
          version: opset13
        - name: onnx
          version: '1'
        - autoSelect: true
          name: tensorflow
          version: '1'
        - autoSelect: true
          name: tensorflow
          version: '2'
        - autoSelect: true
          name: paddle
          version: '2'
        - autoSelect: true
          name: pytorch
          version: '2'
      volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 2Gi
          name: shm
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      annotations:
        serving.kserve.io/deploymentMode: RawDeployment
      labels:
        opendatahub.io/dashboard: 'true'
      name: fraud
    spec:
      predictor:
        maxReplicas: 1
        minReplicas: 1
        model:
          modelFormat:
            name: onnx
            version: '1'
          name: ''
          resources:
            limits:
              cpu: '2'
              memory: 4Gi
            requests:
              cpu: '1'
              memory: 2Gi
          runtime: fraud
          storage:
            key: aws-connection-my-storage
            path: models/fraud/
      transformer:
        containers:
          - name: kserve-container
            image: $(IMAGE_TAG)
            imagePullPolicy: Always
            args:
              - --model_name=fraud
              - --predictor_protocol=v2
              - --predictor_host=$(PREDICTOR_HOST)
              - --predictor_use_ssl=True
parameters:
  - name: PREDICTOR_HOST
    displayName: Predictor Host
    description: The host of the predictor service
    required: true
  - name: IMAGE_TAG
    displayName: Image Tag
    description: Image tag for the transformer
    required: true
    value: quay.io/cfchase/fraud-detection-transformer:latest

